\section{\acl{LSTM}}
\textit{Daniel Andrés López, Frank Reichwein}

Eine weitere Art eine Klassifizierung vorzunehmen ist es ein neuronales Netz zu
verwenden. Dieser Ansatz ist durch die Biologie motiviert und lehnt sich an die
Arbeitsweise eines Gehirns an. Im Gehirn sind Neuronen über Synapsen iteinander
verbunden. Nervenbahnen aus dem gesamten Körper erreichen die Neuronen und
stimulieren sie in unterschiedlicher Intensität. Wird dabei ein Schwellwert
überschritten ist dieses Neuron aktiviert und sendet ebenfalls einen Impuls an
die mit ihm über die Synapsen verbundenen Neuronen. Dies geschieht fortlaufend.
Die Synapsen sind jedoch unterschiedlich stark ausgeprägt, um auch eine
unterschiedliche Stimulierungsintensität weiterzugeben. Dies kommt einer
Gewichtung der Neuronen gleich, da die Eingangssignale einen unterschiedlich
starken Einfluss auf das stimulierte Neuron haben. Die Synapsen sind jedoch
nicht fest und von vornerein vorgegeben. Sie werden kontinuerlich auf- und
abgebaut bzw. verändert. Diesen Vorgang wird im Allgemeinen Lernen bezeichnet.
Die erlebten Erfahrungen werden in dem Netz von Neuronen und Synapsen
verarbeitet und dadurch gespeichert. Erreichen das Gehirn nun neue
Sinneseindrücke werden die Neuronen erneut stimuliert und je nach Ergebnis
werden bestimmte Erinnerungen, Gefühle, Aktionen oder anderes erlebt und
ausgeführt. Jedoch sind sie nicht direkt an ein bestimmtes Ereignis geknüpft,
sondern wurden durch verschiedene Erfahrungen verallgemeinert, um so
verschiedenen Situationen gut verarbeiten zu können. 

\paragraph{Modelle von Neuronen und Synapsen}
Im Bereich des maschinellen Lernen wird versucht die Funktionsweise eines
Gehirns bzw. Netzes aus Neuronen und Synapsen nachzubilden. Dies wird durch
vereinfachte Modelle von Neuroen und Synapsen erreicht. Als Grundlage dient
dabei das Modell von McCulloch und Pitts von 1943 (vgl. \cite{Mcc43}). Es
modelliert ein Neuron mit $n$-vielen Eingabewerten ($x_1,\ldots,x_n$) und einem
Ausgabewert $y$. Die Eingabewerte entsprechen der Aktivierung eines
Vorgängerneurons und der Ausgabewert der Aktivierung des betrachteten Neurons.
\textbf{TODO Picture McCulloch and Pitts Neuron} Die \autoref{} zeigt den Aufbau
eines solchen Neurons. Die Eingaben weren dabei zuerst im \textit{Addierer}
summiert, eine Schwellwertfunktion (bzw. allgemeiner eine Aktivierungsfunktion)
bestimmt dann mit der Summe den Ausgabewert (die Aktivierung) des Neurons. De Weiteren
werden auch die Synapsen nachgebildet in dem jeder Eingabewert bei der
Summierung individuell gewichtet wird. Die Gewichte entsprechen den
unterschiedlich stark ausgeprägten Synapsen. Das klassische Modell erlaubt nur
binäre Ein- und Ausgaben, dies kann jedoch auf Reelle Zahlen erweitert werden,
um unterschiedlich starke Stimulierungen bzw. auch negative Werte (Hemmungen)
zuzulassen.

\paragraph{Neuronale Netzwerke}
Einzelne Neuronen können allerdings kein Gehrin nachbilden. Aus diesem Grund ist
di Kombination mehrerer Neuronen zu einem neuronalen Netz nötig. Im einfachsten
Fall entsteht dabei ein \textit{Perzeptron}. Es werden $n$-viele Neuronen
verwendet. Alle Eingaben werden von allen Neuronen verarbeitet und jedes Neuron
hat eine eigene Aktivierung. Die Gewichte (Synapsen) sind individuell für jedes
Neuron. Die Ausgaben ergeben den Ausgabevektor $\bf{y}$ des Netzwerkes. Um
komplexere Sachverhalte darstellen zu können ist es nötig Neuronen in
Abhängigkeit voneinander zu betrachten. Dies wird in \acp{MLP} vorgenommen.
Mehrere Perzeptrons werden hintereinandergeschaltet, sodass die Ausgabe des
einen Perzeptrons (ein Layer) die Eingabe des nächsten Perzeptrons ist.
Lediglich die Ausgabe des letzten Perzeptrons ist die Ausgabe des Netzwerks. 
Ein Netzwerk kann als gerichteter Graph dargestellt werden. Die Richtung
entspricht dem Datenfluss. Perzeptrons und \acp{MLP} sind azyklische Graphen,
d.h. die Verbindungen von Neuronen gehen nur in Vorwärtsrichtung und bilden
somit keinen Kreis. Daher werden diese Netzwerke \textit{Feedforward}-Netzwerke
genannt. Um besser die Funktionsweise von Gehirnen nachzubilden ist es
notwendig auch Verbidnungen von Neuronen auf sich selbst zuzulassen. Somit
entsteht ein zyklischer Graph und das Netzwerk wird zu einem Rekurrenten
Neuronalen Netz (\acsu{RNN}). 
\textbf{TODO Bilder von Neuronalen Netzen zur Verdeutlichung}

In den folgenden Kapiteln wird eine Sonderform von \acp{RNN} verwendet, ein
\ac{LSTM}. Dazu wird zuerst darauf eingegangen wie ein \ac{LSTM} im
Allgemeinen funktioniert und wie ein solches Netzwerk trainiert werden kann.
Darauf aufbauend wird das Verfahren auf das Projekt angepasst. Es werden die
Datenaufbereitung, die Anpassung des Klassifikators, die Implementierung in
Python, das Training und eine Evaluierung besprochen. 


\subsection{Funktionsweise des Klassifikators (Allgemein)}
 
\subsection{Anwednug auf Projekt}

\subsubsection{Datenaufbereitung}
\label{sec:lstm_data}

In Kapitel \ref{Kapitel 1 TODO} wird beschrieben wie die Daten aufgenommen und
im Allgemeinen verarbeitet werden, bevor sie den Klassifikatoren zur
Verfügung gestellt werden. Diese erhalten jeweils einzelne Frames mit je 64
Daten. Die Daten entprechen dem Frequenzspektrum im Umfeld der
Referenzfrequenz von $18500\text{kHz}$. Eine Geste wurde mit 32 Frames
aufgenommen. Für \ac{LSTM}-Netzwerke sind diese rohen Daten eher ungeignet. Die
Netzwerke können zwar im Allgemeinen mit unverarbeiteteten Daten umgehen,
tortzdem ist ein Vorverarbeitung sinnvoll, da sich dadurch die Verallgemeinerung
des Problems verbessert, die Trainingszeit verkürzt und die benötigten Beispiele
verringern \textbf{Cite}. Aus diesen Gründen wird eine Vorverarbeitung der Daten
vorgenommen. 

Die Beispieldaten sind auf verschiedenen Hardwareplattformen mit
unterschiedlichen Lautstärke und Aufnahmelautstärke Konfigurationen erzeugt
worden. Der Wertebereich dieser Daten ist somit groß und unterschiedlich bei
gleichen Gesten. Das Problem dass durch diese Varianz der Wertemenge entseht
ist, dass \acp{LSTM} Netze (bzw. jede Form von neuronalen Netzen) die Gewichte
entsprechend groß kalibirieren muss, um Unterschiede ausgleichen zu können und
eine verallgemeinertes Ergebnis zu liefern. Da jedoch das (später beschriebene)
Trainigsverfahren iterativ die Gewichte in kleinen Schritten anpasst, führen
große Gewichte zu einem hohen Trainingsaufwand. Um dies zu vermeiden werden die
Daten normalisiert. Konkret wird dabei der Maximalwert eines Frames als
Normierungsfaktor gewählt. Dadurch liegen alle Werte im Intervall $[0,1]$.
Eine Transformation \textbf{richtiges Wort?} des SPektrums hin zu den
Veränderungen der Spektrums im Vergleich zum Ruhespektrum wird ebenfalls
vorgenommen. Dies wird aufgrund durchgeführter Tests vorgenommen, die eine
bessere Klassifizierungen für die transformierten Daten bescheinigen
(\textbf{Tests einbinden}). Der allgemeine Ruhezustand wird aus den Beispielen
des Ruhzustands (Geste 6 und 7) durch das normalisierte Mittel gewonnen. Der
Ruhezustand wird dann von jedem Beispiel abgezogen. Es werden hierbei lediglich
die Beispiele verwendet, denkbar ist jedoch auch die Aufnahme des Ruhezustands
auf dem Zielgerät, um spezifischere Daten zu erhalten. 

Bei der Betrachtung der Daten fällt auf, dass der Doppler-Effekt in einem
kleinen Intervall um die Referenzfrequenz auftritt und die Daten außerhalb
dieses Intervalls keine große Bedeutung spielen. Aus diesem Grund ist eine
Verkleinerung der Frame Größe in Betracht zu ziehen. Da jedoch eine manuelle
Betrachtung aller Daten nicht möglich ist, lässt sich diese Beobachtung nicht
verifzieren. Daher werden mehrere \ac{LSTM}-Netze mit unterschiedlichen
Inputframelängen trainiert und die Ergebnisse verglichen. \textbf{TODO Tabelle
mit Ergebnissen} Erstaunlicherweise ist die aufgestellte Vermutung falsch und die
Netzwerke ohne zugeschnittenen Trainingsdaten klassifizieren besser. 

Eine weitere Beobachtung die gemacht werden kann ist, dass benachbarte
Datenpunkte sich nicht wesentlich unterscheiden. Dies kann genutzt werden um die
Eingabedimension zu verringern, indem z.B. jeweils zwei benachbarte Datenpunkte
miteinander addiert werden. Dies kann auch mit dem verkleinern der Frames
kombiniert werden. Um den Einfluss auf das Klassifierzungsverhalten von
\ac{LSTM}-Netzen zu zeigen sind auch hier Tests durchgeführt worden.
\textbf{TODO Tabelle mit Ergebnissen} Das Ergebnis zeigt, dass die Netze ohne
die Summe benachbarter Werte besser klassifizieren als mit der Summe. 

Weitere Methoden der Datenvorverarbeitung wurden ebenfalls untersucht, jedoch
hat dies die Ergebnisse ebenfalls negativ beeinflusst, weshalb sie nicht näher
betrachtet wurden. Darunter fällt u.a. die statische Eliminierung von Rauschen.
Diese Methode setzt jeden Wert der unter einem bestimmten Schwellwert ist zu 0,
da dies kein signifikanter Beitrag zur Geste ist sondern ein Rauschen. Das
Ergebnis dieser Untersuchung ist jedoch unbefridigend, sodass die Idee nicht
genutzt wird. \textbf{TODO Messwerte?}



\subsubsection{Anpassung des Klassifikators}
\subsubsection{Implementierung}

\subsubsection{Training}

\subsubsection{Evaluation}

\subsection{Fazit}


\nocite{schaul2010,GERS2001,WIKI2013,Schmidhuber2013,LSTM1,Nerbonne1,Hochreiter:1997}
