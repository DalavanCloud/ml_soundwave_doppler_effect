\section{\acl{LSTM}}
\textit{Daniel Andrés López, Frank Reichwein}

Eine weitere Art eine Klassifizierung vorzunehmen ist es ein neuronales Netz zu
verwenden. Dieser Ansatz ist durch die Biologie motiviert und lehnt sich an die
Arbeitsweise eines Gehirns an. Im Gehirn sind Neuronen über Synapsen iteinander
verbunden. Nervenbahnen aus dem gesamten Körper erreichen die Neuronen und
stimulieren sie in unterschiedlicher Intensität. Wird dabei ein Schwellwert
überschritten ist dieses Neuron aktiviert und sendet ebenfalls einen Impuls an
die mit ihm über die Synapsen verbundenen Neuronen. Dies geschieht fortlaufend.
Die Synapsen sind jedoch unterschiedlich stark ausgeprägt, um auch eine
unterschiedliche Stimulierungsintensität weiterzugeben. Dies kommt einer
Gewichtung der Neuronen gleich, da die Eingangssignale einen unterschiedlich
starken Einfluss auf das stimulierte Neuron haben. Die Synapsen sind jedoch
nicht fest und von vornerein vorgegeben. Sie werden kontinuerlich auf- und
abgebaut bzw. verändert. Diesen Vorgang wird im Allgemeinen Lernen bezeichnet.
Die erlebten Erfahrungen werden in dem Netz von Neuronen und Synapsen
verarbeitet und dadurch gespeichert. Erreichen das Gehirn nun neue
Sinneseindrücke werden die Neuronen erneut stimuliert und je nach Ergebnis
werden bestimmte Erinnerungen, Gefühle, Aktionen oder anderes erlebt und
ausgeführt. Jedoch sind sie nicht direkt an ein bestimmtes Ereignis geknüpft,
sondern wurden durch verschiedene Erfahrungen verallgemeinert, um so
verschiedenen Situationen gut verarbeiten zu können. 

\paragraph{Modelle von Neuronen und Synapsen}
Im Bereich des maschinellen Lernen wird versucht die Funktionsweise eines
Gehirns bzw. Netzes aus Neuronen und Synapsen nachzubilden. Dies wird durch
vereinfachte Modelle von Neuroen und Synapsen erreicht. Als Grundlage dient
dabei das Modell von McCulloch und Pitts von 1943 (vgl. \cite{Mcc43}). Es
modelliert ein Neuron mit $n$-vielen Eingabewerten ($x_1,\ldots,x_n$) und einem
Ausgabewert $y$. Die Eingabewerte entsprechen der Aktivierung eines
Vorgängerneurons und der Ausgabewert der Aktivierung des betrachteten Neurons.
\textbf{TODO Picture McCulloch and Pitts Neuron} Die \autoref{} zeigt den Aufbau
eines solchen Neurons. Die Eingaben weren dabei zuerst im \textit{Addierer}
summiert, eine Schwellwertfunktion (bzw. allgemeiner eine Aktivierungsfunktion)
bestimmt dann mit der Summe den Ausgabewert (die Aktivierung) des Neurons. De Weiteren
werden auch die Synapsen nachgebildet in dem jeder Eingabewert bei der
Summierung individuell gewichtet wird. Die Gewichte entsprechen den
unterschiedlich stark ausgeprägten Synapsen. Das klassische Modell erlaubt nur
binäre Ein- und Ausgaben, dies kann jedoch auf Reelle Zahlen erweitert werden,
um unterschiedlich starke Stimulierungen bzw. auch negative Werte (Hemmungen)
zuzulassen.

\paragraph{Neuronale Netzwerke}
Einzelne Neuronen können allerdings kein Gehrin nachbilden. Aus diesem Grund ist
di Kombination mehrerer Neuronen zu einem neuronalen Netz nötig. Im einfachsten
Fall entsteht dabei ein \textit{Perzeptron}. Es werden $n$-viele Neuronen
verwendet. Alle Eingaben werden von allen Neuronen verarbeitet und jedes Neuron
hat eine eigene Aktivierung. Die Gewichte (Synapsen) sind individuell für jedes
Neuron. Die Ausgaben ergeben den Ausgabevektor $\bf{y}$ des Netzwerkes. Um
komplexere Sachverhalte darstellen zu können ist es nötig Neuronen in
Abhängigkeit voneinander zu betrachten. Dies wird in \acp{MLP} vorgenommen.
Mehrere Perzeptrons werden hintereinandergeschaltet, sodass die Ausgabe des
einen Perzeptrons (ein Layer) die Eingabe des nächsten Perzeptrons ist.
Lediglich die Ausgabe des letzten Perzeptrons ist die Ausgabe des Netzwerks. 
Ein Netzwerk kann als gerichteter Graph dargestellt werden. Die Richtung
entspricht dem Datenfluss. Perzeptrons und \acp{MLP} sind azyklische Graphen,
d.h. die Verbindungen von Neuronen gehen nur in Vorwärtsrichtung und bilden
somit keinen Kreis. Daher werden diese Netzwerke \textit{Feedforward}-Netzwerke
genannt. Um besser die Funktionsweise von Gehirnen nachzubilden ist es
notwendig auch Verbidnungen von Neuronen auf sich selbst zuzulassen. Somit
entsteht ein zyklischer Graph und das Netzwerk wird zu einem Rekurrenten
Neuronalen Netz (\acsu{RNN}). 
\textbf{TODO Bilder von Neuronalen Netzen zur Verdeutlichung}

In den folgenden Kapiteln wird eine Sonderform von \acp{RNN} verwendet, ein
\ac{LSTM}. Dazu wird zuerst darauf eingegangen wie ein \ac{LSTM} im
Allgemeinen funktioniert und wie ein solches Netzwerk trainiert werden kann.
Darauf aufbauend wird das Verfahren auf das Projekt angepasst. Es werden die
Datenaufbereitung, die Anpassung des Klassifikators, die Implementierung in
Python, das Training und eine Evaluierung besprochen. 


\subsection{Funktionsweise des Klassifikators (Allgemein)}
 
\subsection{Anwednug auf Projekt}
TODO diagramm des netztes


\subsubsection{Datenaufbereitung}
\label{sec:lstm_data}

In Kapitel \ref{Kapitel 1 TODO} wird beschrieben wie die Daten aufgenommen und
im Allgemeinen verarbeitet werden, bevor sie den Klassifikatoren zur
Verfügung gestellt werden. Diese erhalten jeweils einzelne Frames mit je 64
Daten. Die Daten entprechen dem Frequenzspektrum im Umfeld der
Referenzfrequenz von $18500\text{kHz}$. Eine Geste wurde mit 32 Frames
aufgenommen. Für \ac{LSTM}-Netzwerke sind diese rohen Daten eher ungeignet. Die
Netzwerke können zwar im Allgemeinen mit unverarbeiteteten Daten umgehen,
tortzdem ist ein Vorverarbeitung sinnvoll, da sich dadurch die Verallgemeinerung
des Problems verbessert, die Trainingszeit verkürzt und die benötigten Beispiele
verringern \textbf{Cite}. Aus diesen Gründen wird eine Vorverarbeitung der Daten
vorgenommen. 

Die Beispieldaten sind auf verschiedenen Hardwareplattformen mit
unterschiedlichen Lautstärke und Aufnahmelautstärke Konfigurationen erzeugt
worden. Der Wertebereich dieser Daten ist somit groß und unterschiedlich bei
gleichen Gesten. Das Problem dass durch diese Varianz der Wertemenge entseht
ist, dass \acp{LSTM} Netze (bzw. jede Form von neuronalen Netzen) die Gewichte
entsprechend groß kalibirieren muss, um Unterschiede ausgleichen zu können und
eine verallgemeinertes Ergebnis zu liefern. Da jedoch das (später beschriebene)
Trainigsverfahren iterativ die Gewichte in kleinen Schritten anpasst, führen
große Gewichte zu einem hohen Trainingsaufwand. Um dies zu vermeiden werden die
Daten normalisiert. Konkret wird dabei der Maximalwert eines Frames als
Normierungsfaktor gewählt. Dadurch liegen alle Werte im Intervall $[0,1]$.
Eine Transformation \textbf{richtiges Wort?} des SPektrums hin zu den
Veränderungen der Spektrums im Vergleich zum Ruhespektrum wird ebenfalls
vorgenommen. Dies wird aufgrund durchgeführter Tests vorgenommen, die eine
bessere Klassifizierungen für die transformierten Daten bescheinigen
(\textbf{Tests einbinden}). Der allgemeine Ruhezustand wird aus den Beispielen
des Ruhzustands (Geste 6 und 7) durch das normalisierte Mittel gewonnen. Der
Ruhezustand wird dann von jedem Beispiel abgezogen. Es werden hierbei lediglich
die Beispiele verwendet, denkbar ist jedoch auch die Aufnahme des Ruhezustands
auf dem Zielgerät, um spezifischere Daten zu erhalten. 

Bei der Betrachtung der Daten fällt auf, dass der Doppler-Effekt in einem
kleinen Intervall um die Referenzfrequenz auftritt und die Daten außerhalb
dieses Intervalls keine große Bedeutung spielen. Aus diesem Grund ist eine
Verkleinerung der Frame Größe in Betracht zu ziehen. Da jedoch eine manuelle
Betrachtung aller Daten nicht möglich ist, lässt sich diese Beobachtung nicht
verifzieren. Daher werden mehrere \ac{LSTM}-Netze mit unterschiedlichen
Inputframelängen trainiert und die Ergebnisse verglichen. \textbf{TODO Tabelle
mit Ergebnissen} Erstaunlicherweise ist die aufgestellte Vermutung falsch und die
Netzwerke ohne zugeschnittenen Trainingsdaten klassifizieren besser. 

Eine weitere Beobachtung die gemacht werden kann ist, dass benachbarte
Datenpunkte sich nicht wesentlich unterscheiden. Dies kann genutzt werden um die
Eingabedimension zu verringern, indem z.B. jeweils zwei benachbarte Datenpunkte
miteinander addiert werden. Dies kann auch mit dem verkleinern der Frames
kombiniert werden. Um den Einfluss auf das Klassifierzungsverhalten von
\ac{LSTM}-Netzen zu zeigen sind auch hier Tests durchgeführt worden.
\textbf{TODO Tabelle mit Ergebnissen} Das Ergebnis zeigt, dass die Netze ohne
die Summe benachbarter Werte besser klassifizieren als mit der Summe. 

Weitere Methoden der Datenvorverarbeitung wurden ebenfalls untersucht, jedoch
hat dies die Ergebnisse ebenfalls negativ beeinflusst, weshalb sie nicht näher
betrachtet wurden. Darunter fällt u.a. die statische Eliminierung von Rauschen.
Diese Methode setzt jeden Wert der unter einem bestimmten Schwellwert ist zu 0,
da dies kein signifikanter Beitrag zur Geste ist sondern ein Rauschen. Das
Ergebnis dieser Untersuchung ist jedoch unbefridigend, sodass die Idee nicht
genutzt wird. \textbf{TODO Messwerte?}



\subsubsection{Anpassung des Klassifikators}
\subsubsection{Implementierung}
Da es zum Ende des Projektes ein Programm geben soll, indem alle Klassifikatoren 
eingebunden sind, haben sich alle Teilnehmer auf ein Interface geeingtt, was 
jeder Klassifikator implementieren muss. Dies ist die abstrakte Klasse \textit{IClassifier}.

\textbf{TODO Klassendiagramm}

Die Implementation des \ac{LSTM}-Klassifikators ist in der Klasse LSTM zu finden.
Für diese wurde \cite{PyBrain} verwendet. Diese Bibliothek stellt verschiedene
Machine Learning Algorithmen bereit, unter anderem Neuronale Netzwerke mit 
\ac{LSTM}-Neuronen.
Außerdem wurden in der Datei util verschiedene Hilfsfunktionen erstellt.
Des weiteren werden Konfigurationseinstellungen in einer Datei gespeichert und zur 
initialisierung der geladen. 
Da beabsichtigt ist mit dem Programm den PC zu steuern, wurde die Python Bibliothek 
\cite{Python-uinput} eingebunden. Mit dieser können Keycodes an den Kernel geschickt werden.
Die Bibliothek funktioniert nur unter Linux und erfordert Root-Rechte.
Im folgenden wird auf die Implementation der einzelnen MEthoden der 
\textit{IClassifier}-Klasse eingegangen.

\subsubsection*{classify}
Die \textit{classify}-Methode dient der Live-Klassifikation. Der Methode wird 
der Datensatz der aktuellen Aufnahme übergeben. Dieser enthält ein Array 
mit 64-Datenpunkten. Die übergebenen Daten werden Normalisiert und anschließend 
wird der Durchschnitt abgezogen, wie in Kapitel \ref{sec:lstm_data} beschrieben.
Nachdem die Daten vorbearbeitet wurden, werden diese einer Classify-Methode 
übergeben. Es wurden zwei verschiede Ideen implementiert.

\begin{lstlisting}[language=Python,caption={Classify
Variante 1},label={lst:lstm_classify1}]{lst:lstm_classify1}
def __classify1(self,data):
	self.datalist.append(data)
	self.datanum += 1
	if(self.datanum % 32 == 0):
		self.net.reset()
		out = self._activateSequence(self.datalist)
		print(str(out))
		self.datalist = []
		self.datanum = 0
		return out
	return -1
\end{lstlisting}

Die Methode \textit{classify1} speichert so lange die übergebenen Datenwerte, 
bis 32-Werte aufgenommen wurden. Diese 32-Werten werden dem \ac{LSTM}-Netz 
übergeben und eine Klassifikation gestartet.

\begin{lstlisting}[language=Python,caption={Classify
Variante 2},label={lst:lstm_classify2}]{lst:lstm_classify2}
def __classify2(self, data):
	self.datanum += 1
	self.datalist.append(data)
	if(self.datanum % 32 == 0):
		self.has32 = True
	if(self.has32):
		self.net.reset()
		Y_pred = self._activateSequence(self.datalist)
		del self.datalist[0]
		self.predHistory[0] = Y_pred
		self.predHistory = np.roll(self.predHistory, -1)
		expected = stats.mode(self.predHistory, 0)
		if(expected[1][0] >= self.predHistHalfUpper):
			if(int(expected[0][0]) != self.previouspredict):
				oldPrevious, oldPredCounter = self.previouspredict, self.previouspredict
				self.previouspredict = int(expected[0][0])
				self.predcounter = 1
				return oldPrevious, oldPredCounter
			else:
				self.predcounter += 1
				if(self.predcounter == 4):
					print(str(self.previouspredict))
					self.outkeys.outForClass(self.previouspredict)
				return self.previouspredict, self.predcounter
	return -1, -1
\end{lstlisting}

Die Methode \textit{classify2} speichert so lange die übergebenen Datenwerte, 
bis 32-Werte aufgenommen wurden. Diese 32-Werten werden dem \ac{LSTM}-Netz 
übergeben und eine Klassifikation gestartet. Die erkannte Geste wird in einer 
Liste gespeichert. Von dieser Liste wird, mittels der Python-Funktion mode, 
der Wert ermittelt der am häufigsten vorkommt. Ist dieser Wert größer als eine 
definierte Schranke wird er in \textit{previouspredict} gespeichert. Wenn vier 
mal hinter einander die gleiche Geste erkannt wurde, wird diese ausgegeben.

\subsubsection{Training}
python dataset
trainingsalgo

\subsubsection{Evaluation}

\subsection{Fazit}


\nocite{schaul2010,GERS2001,WIKI2013,Schmidhuber2013,LSTM1,Nerbonne1,Hochreiter:1997}
