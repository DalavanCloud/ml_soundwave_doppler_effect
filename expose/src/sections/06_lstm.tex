\section{\acl{LSTM}}

\subsection{Einleitung}
\ac{LSTM} ist ein \ac{RNN}, welches die Eigenschaft hat Zustände über einen
bestimmten Zeitraum zu behalten und danach wieder zu vergessen. \acp{LSTM} sind
somit eine spezielle Form von \acp{MLP}. Ein \ac{LSTM} definiert rekurrente
Verbindungen (Verbindungen von Neuronen zu vorhergehenden bzw. denselben
Neuronen) in der Art und Weise, dass die Eingabe Daten bzw.
die Ausgabe Daten gespeichert über mehrere Aufrufe des Netzwerkes hinaus
gespeichert werden. Diese gespeicherten Daten werden bei bestimmten
Eingangsdaten Ausgegeben, bei anderen \textit{vergessen} bzw. überschrieben oder
nicht verwendet. 
 
\acp{RNN} und vielmehr noch \acp{LSTM} sind Modelle für das menschliche Gehirn.
Im Gehirn sind viele Neuronen untereinander, also mit sich selbst, nachfolgenden
und vorhergehenden Nueronen vernetzt, sogenannten Feedback Verbindungen. Jedes
Neueron ist dabei ein speziell aufgebaut um Informationen zu speichern.
\acp{RNN} haben keine bzw. einfache Erinnerungsneuronen und sind aus diesem
Grund nicht für länger Erinnerungen geeignet. Bei \acp{LSTM} hingegen sind
komplexe Erinnerungsneuronen (\ac{LSTM}-Neuronen) miteinander verknüpft. Jedes
einzelne Neuron kann Informationen über einen längeren Zeitraum, d.h. auch bei
häufiger Nutzung des Netzes, Daten speichern. Dadurch dass \acp{LSTM} Daten auch
nach dem Training speichern können, also verschiedene Zustände haben,
unterscheiden sie sich von vielen anderen Machine Learning Verfahren, wie z.B.
\ac{HMM} oder \acp{SVM}. Sie sind besonders für komplexe Aufgaben mit unscharfen
Beobachtungen geeignet und werden bevorzugt in Handschrift- und Spracherkennung,
Proteinanalyse und weiteren dynamischen Feldern verwendet. Durch die Analogie
zum Gehirn können \acp{LSTM} biologisch motiviert werden.
 
\subsection{Architektur von \aclp{LSTM}}
Ein \ac{RNN} mit \ac{LSTM} Einheiten hat ein Hiddenlayer in dem die
verschiedenen \ac{LSTM} Knoten enthalten sind. Das Outputlayer ist dann ein
einfaches lineares oder sigmoidales Layer.  
\begin{figure}[h]
	\begin{center}
	\includegraphics[scale=0.5]{lstm_block}
	\caption{Schema eines \acs{LSTM} Blocks}
	Quelle: \cite{WIKI2013}
	\label{fig:lstm_block}
	\end{center}
\end{figure}


\subsection{Funktionsweise von der \acs{PyBrain} \acs{LSTM} Implementierung}
\ac{PyBrain}\cite{schaul2010} ist eine Bibliothek für Python und stellt
verschiedene Module aus dem Bereich des maschinellen Lernes bereit. Dabei
spielen neuronale Netzwerke eine zentrale Rolle, diese werden in verschiedenen
Lernmethoden angewendet wie z.B. im Reinforcement, unüberwachten oder
evolutionären Lernen.

\begin{lstlisting}
net = buildNetwork(INPUTS, HIDDEN, OUTPUTS, hiddenclass=LSTMLayer, outclass=SigmoidLayer, recurrent=True, bias=True) 
ds = DATASET 
trainer = BackpropTrainer(net, ds)
for _ in range(1000):
    trainer.train()
\end{lstlisting}
 
Im Quellcode~\ref{lst:} ist ein Beispiel aufgezeigt wie \ac{PyBrain} verwendet
werden kann. Es gibt dabei verschiedene Möglichkeiten im Aufbau des Netzes
(v.a. Anzahl der Neuronen und \ac{LSTM}-Blocks), in der Datenhaltung
(\ref{sec:lstm_data:}) sowie in der Trainingsmethode (z.B. Backpropagation). 
\subsection{Verwendetes Datenmodell}
\label{sec:lstm_data}
 
\nocite{GERS2001,WIKI2013,Schmidhuber2013,LSTM1,Nerbonne1}
